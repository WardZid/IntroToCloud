{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzUD1/2WGUnHjK0wMvFcMr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WardZid/IntroToCloud/blob/main/TencentScraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "מבוא למחשוב ענן | שבוע 6"
      ],
      "metadata": {
        "id": "KO56H-pYeJ0D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOSYmDadeFYq"
      },
      "outputs": [],
      "source": [
        "!pip install requests beautifulsoup4\n",
        "!pip install firebase"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prepare Firebase\n",
        "from firebase import firebase\n",
        "fbConn = firebase.FirebaseApplication('https://braudecloud-18-02-2024-default-rtdb.europe-west1.firebasedatabase.app/',None)"
      ],
      "metadata": {
        "id": "cQQhN5Wc7V4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def fetch_page(url):\n",
        "  response = requests.get(url)\n",
        "  if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    return soup\n",
        "  else:\n",
        "    return None\n",
        "\n"
      ],
      "metadata": {
        "id": "JoBow6lLfCUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def index_words(soup):\n",
        "    index = {}\n",
        "    words = re.findall(r'\\w+', soup.get_text())\n",
        "    for word in words:\n",
        "        word = word.lower()\n",
        "        if word in index:\n",
        "            index[word] += 1\n",
        "        else:\n",
        "            index[word] = 1\n",
        "    #print(format_json(index))\n",
        "    return index\n"
      ],
      "metadata": {
        "id": "0c4x4CZxfZv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words(index):\n",
        "  stop_words = {'a', 'an', 'the', 'and', 'or','in', 'on', 'at'}\n",
        "  for stop_word in stop_words:\n",
        "      if stop_word in index:\n",
        "            del index[stop_word]\n",
        "  return index\n"
      ],
      "metadata": {
        "id": "_ukCou8LfrtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "def apply_stemming(index):\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_index = {}\n",
        "    for word, count in index.items():\n",
        "        stemmed_word = stemmer.stem(word)\n",
        "        if stemmed_word in stemmed_index:\n",
        "            stemmed_index[stemmed_word] += count\n",
        "        else:\n",
        "            stemmed_index[stemmed_word] = count\n",
        "    return stemmed_index\n"
      ],
      "metadata": {
        "id": "kI2TrhzOgEg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search(query, index):\n",
        "    query_words = re.findall(r'\\w+', query.lower())\n",
        "    results = {}\n",
        "    for word in query_words:\n",
        "        if word in index:\n",
        "            results[word] = index[word]\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "9Nf5rkykghRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_engine(url, query):\n",
        "    soup = fetch_page(url)\n",
        "    if soup is None:\n",
        "        return None\n",
        "    index = index_words(soup)\n",
        "    index = remove_stop_words(index)\n",
        "    index = apply_stemming(index)\n",
        "    results = search(query, index)\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "g4aQvkHYgtVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.parse import urlparse, urljoin\n",
        "from collections import deque\n",
        "\n",
        "# Function to extract links from a webpage\n",
        "def extract_links(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    links = []\n",
        "    for link in soup.find_all('a', href=True):\n",
        "        links.append(link['href'])\n",
        "    return links\n",
        "\n",
        "# Function to check if links are from the same domain\n",
        "def check_same_domain(base_url, links):\n",
        "    base_domain = urlparse(base_url).netloc\n",
        "    same_domain_links = []\n",
        "    for link in links:\n",
        "        parsed_link = urlparse(link)\n",
        "        if parsed_link.netloc == base_domain or parsed_link.netloc == '':\n",
        "            same_domain_links.append(link)\n",
        "    return same_domain_links\n",
        "\n",
        "def is_same_domain(base_url, url):\n",
        "    base_domain = urlparse(base_url).netloc\n",
        "    link_domain = urlparse(url).netloc\n",
        "    return base_domain == link_domain"
      ],
      "metadata": {
        "id": "Bf44ZK1FjCIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_page_title(url):\n",
        "    soup = fetch_page(url)\n",
        "    if soup:\n",
        "        title_tag = soup.find('title')\n",
        "        if title_tag:\n",
        "            return title_tag.get_text()\n",
        "    return None"
      ],
      "metadata": {
        "id": "bzYrZoVMyRz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "\n",
        "def hash_url(url_to_hash):\n",
        "  return hashlib.md5(url_to_hash.encode()).hexdigest()\n",
        "\n",
        "def save_and_hash_url(url_to_hash_save):\n",
        "  # Hash the URL using MD5 hashing algorithm\n",
        "  hashed_url = hash_url(url_to_hash_save)\n",
        "  title = fetch_page_title(url_to_hash_save)\n",
        "  data_to_upload = {\n",
        "    'url': url_to_hash_save,\n",
        "    'title': title\n",
        "  }\n",
        "  result = fbConn.put('/pages', hashed_url,data=data_to_upload)\n",
        "  if result is None:\n",
        "      print(\"save_and_hash_url - None\")\n",
        "  #print(\"save_and_hash \",result)\n",
        "  return hashed_url"
      ],
      "metadata": {
        "id": "onoez-DRtEn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def save_links_pointing(url_pointing,links_pointed_at):\n",
        "  for link in links_pointed_at:\n",
        "    hashed_link = hash_url(link)\n",
        "    result = fbConn.get(\"indx/\"+ hashed_link,None)\n",
        "    #print(hashed_link,\" \",result)\n",
        "    if result == None:\n",
        "      save_and_hash_url(link)\n",
        "\n",
        "    result = fbConn.post_async(\"pages/\"+hashed_link+\"/\"+\"pointed_from\", data=hash_url(url_pointing))\n",
        "    if result is None:\n",
        "      print(\"save_links_pointing - None\")\n",
        "    print(\"point \",result)"
      ],
      "metadata": {
        "id": "1cuO6P4nw3Be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.parse\n",
        "\n",
        "def search_words(url,query_words):\n",
        "  hashed_url = save_and_hash_url(url)\n",
        "  #print(\"search_words\",hashed_url)\n",
        "  for query in query_words:\n",
        "    result = search_engine(url, query)\n",
        "    if result == {} or result is None or result[query] is None:\n",
        "      continue\n",
        "    data_to_upload = {\n",
        "        'count': result[query]\n",
        "    }\n",
        "    #print(hashed_url,\" \",query,\" \",data_to_upload)\n",
        "    result = fbConn.put('/index/' + query,hashed_url,data=data_to_upload)\n",
        "    if result is None:\n",
        "      print(\"search_words - None\")\n",
        "    #print(\"word \",result)"
      ],
      "metadata": {
        "id": "qrxcWy4_u1PV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_words_from_firestore():\n",
        "    data = fbConn.get('index',None)\n",
        "    keys = list(data.keys()) if data else []\n",
        "    return keys"
      ],
      "metadata": {
        "id": "1d5tam8wiDLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def scrape_bfs(start_url):\n",
        "\n",
        "  visited = set()\n",
        "  # Initialize a queue with first url\n",
        "  queue = deque([start_url])\n",
        "\n",
        "  queries = get_words_from_firestore()\n",
        "  print(queries)\n",
        "  ####queries = [\"tencent\",\"cloud\",\"games\",\"studio\",\"holdings\",\"business\",\"china\",\"nvidia\",\"graphics\",\"development\"]\n",
        "  #get the base domain to start from\n",
        "  #base_domain = urlparse(start_url).netloc\n",
        "\n",
        "  # Loop until the queue is empty\n",
        "  while queue:\n",
        "    #get next url to search\n",
        "    url = queue.popleft()\n",
        "    # Skip if the URL has been visited before\n",
        "    if url in visited:\n",
        "        continue\n",
        "\n",
        "    # mark visited\n",
        "    visited.add(url)\n",
        "\n",
        "    #Get all the links in this page\n",
        "    links = extract_links(url)\n",
        "    print(\"Visiting: \",url,\"\\nLinks Extracted: \",len(links))\n",
        "\n",
        "    #searching words and saving them to firebase\n",
        "    search_words(url,queries)\n",
        "\n",
        "    #threading.Thread(target=save_links_pointing, args=(url, links)).start()\n",
        "    #save_links_pointing(url,links)\n",
        "\n",
        "    # Process link\n",
        "    for link in links:\n",
        "      absolute_link = urljoin(url, link)\n",
        "\n",
        "      if is_same_domain(start_url, absolute_link) and absolute_link not in visited:\n",
        "        #add link to queue\n",
        "        queue.append(absolute_link)\n",
        "\n"
      ],
      "metadata": {
        "id": "HUFaVvn3g400"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scrape_bfs('https://www.tencent.com/en-us/')"
      ],
      "metadata": {
        "id": "Aw4gTsFGs2pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(hash_url('https://www.tencent.com/en-us/'))"
      ],
      "metadata": {
        "id": "OmAR_2ejxetb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}