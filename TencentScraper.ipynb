{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwOYycKphyrzt1t5vfqbqA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WardZid/IntroToCloud/blob/main/TencentScraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Single Website Scraper\n",
        "___\n",
        "This Notebook Scrapes a given website, given a starting url.\n",
        "<br>The Scraper works by retreiving  list of words to search for from the database, and counts the occurances of these words in the the pages of the website.\n",
        "<br>How it works:\n",
        "\n",
        "\n",
        "1. Given a starting url\n",
        "2. Fetches words to search for from DB\n",
        "3. Searches for these words in the page\n",
        "4. Saves the counts of occurances for each word\n",
        "5. Retrieves all the links in the page that navigate to same domain as the current url\n",
        "6. Saves current url to pointed link. Each link holds a list of urls that navigate to it\n",
        "7. Repeat steps 2 to 6 for all the found links whilst avoiding previously visited links.\n",
        "\n",
        "This approach is based on the BFS algorithm where we prioritize breadth first since more shallow pages of a website are usually the most relevant.\n"
      ],
      "metadata": {
        "id": "KO56H-pYeJ0D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOSYmDadeFYq",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Install dependancies\n",
        "\n",
        "!pip install requests beautifulsoup4\n",
        "!pip install firebase"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Initialize Firebase Connection\n",
        "\n",
        "from firebase import firebase\n",
        "fbConn = firebase.FirebaseApplication('https://braudecloud-18-02-2024-default-rtdb.europe-west1.firebasedatabase.app/',None)\n",
        "#fbConn = firebase.FirebaseApplication('https://smart-howl-250311.firebaseio.com/',None)"
      ],
      "metadata": {
        "id": "cQQhN5Wc7V4v",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Fetch page contents using BeautifulSoup\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def fetch_page(url):\n",
        "  response = requests.get(url)\n",
        "  if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    return soup\n",
        "  else:\n",
        "    return None\n",
        "\n"
      ],
      "metadata": {
        "id": "JoBow6lLfCUf",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Index the words\n",
        "\n",
        "import re\n",
        "def index_words(soup):\n",
        "    index = {}\n",
        "    words = re.findall(r'\\w+', soup.get_text())\n",
        "    for word in words:\n",
        "        word = word.lower()\n",
        "        if word in index:\n",
        "            index[word] += 1\n",
        "        else:\n",
        "            index[word] = 1\n",
        "    #print(format_json(index))\n",
        "    return index\n"
      ],
      "metadata": {
        "id": "0c4x4CZxfZv0",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Remove unwanted Stop Words\n",
        "\n",
        "def remove_stop_words(index):\n",
        "  stop_words = {'a', 'an', 'the', 'and', 'or','in', 'on', 'at'}\n",
        "  for stop_word in stop_words:\n",
        "      if stop_word in index:\n",
        "            del index[stop_word]\n",
        "  return index\n"
      ],
      "metadata": {
        "id": "_ukCou8LfrtZ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Stem the words to extract the root word\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "def apply_stemming(index):\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_index = {}\n",
        "    for word, count in index.items():\n",
        "        stemmed_word = stemmer.stem(word)\n",
        "        if stemmed_word in stemmed_index:\n",
        "            stemmed_index[stemmed_word] += count\n",
        "        else:\n",
        "            stemmed_index[stemmed_word] = count\n",
        "    return stemmed_index\n"
      ],
      "metadata": {
        "id": "kI2TrhzOgEg3",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Search the page for a query word\n",
        "\n",
        "def search(query, index):\n",
        "    query_words = re.findall(r'\\w+', query.lower())\n",
        "    results = {}\n",
        "    for word in query_words:\n",
        "        if word in index:\n",
        "            results[word] = index[word]\n",
        "    return results\n",
        "\n",
        "\n",
        "def search_engine(url, query):\n",
        "    soup = fetch_page(url)\n",
        "    if soup is None:\n",
        "        return None\n",
        "    index = index_words(soup)\n",
        "    index = remove_stop_words(index)\n",
        "    index = apply_stemming(index)\n",
        "    results = search(query, index)\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "g4aQvkHYgtVz",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Extract all links from the page and ensure only the ones in the same domain as the original page\n",
        "\n",
        "from urllib.parse import urlparse, urljoin\n",
        "from collections import deque\n",
        "\n",
        "# Function to extract links from a webpage\n",
        "def extract_links(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    links = []\n",
        "    for link in soup.find_all('a', href=True):\n",
        "        links.append(link['href'])\n",
        "    return links\n",
        "\n",
        "# Function to check if links are from the same domain\n",
        "def check_same_domain(base_url, links):\n",
        "    base_domain = urlparse(base_url).netloc\n",
        "    same_domain_links = []\n",
        "    for link in links:\n",
        "        parsed_link = urlparse(link)\n",
        "        if parsed_link.netloc == base_domain or parsed_link.netloc == '':\n",
        "            same_domain_links.append(link)\n",
        "    return same_domain_links\n",
        "\n",
        "def is_same_domain(base_url, url):\n",
        "    base_domain = urlparse(base_url).netloc\n",
        "    link_domain = urlparse(url).netloc\n",
        "    return base_domain == link_domain"
      ],
      "metadata": {
        "id": "Bf44ZK1FjCIh",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Pull the title of the page to view it in the search results\n",
        "def fetch_page_title(url):\n",
        "    soup = fetch_page(url)\n",
        "    if soup:\n",
        "        title_tag = soup.find('title')\n",
        "        if title_tag:\n",
        "            return title_tag.get_text()\n",
        "    return None"
      ],
      "metadata": {
        "id": "bzYrZoVMyRz8",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Save URLs\n",
        "\n",
        "import hashlib\n",
        "\n",
        "def hash_url(url_to_hash):\n",
        "  return hashlib.md5(url_to_hash.encode()).hexdigest()\n",
        "\n",
        "def save_and_hash_url(url_to_hash_save):\n",
        "  # Hash the URL using MD5 hashing algorithm\n",
        "  hashed_url = hash_url(url_to_hash_save)\n",
        "  title = fetch_page_title(url_to_hash_save)\n",
        "  data_to_upload = {\n",
        "    'url': url_to_hash_save,\n",
        "    'title': title\n",
        "  }\n",
        "  result = fbConn.put('/pages', hashed_url,data=data_to_upload)\n",
        "  if result is None:\n",
        "      print(\"save_and_hash_url - None\")\n",
        "  #print(\"save_and_hash \",result)\n",
        "  return hashed_url"
      ],
      "metadata": {
        "id": "onoez-DRtEn1",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Search for words in a page and save the results\n",
        "import urllib.parse\n",
        "\n",
        "def search_words(url,query_words):\n",
        "  hashed_url = save_and_hash_url(url)\n",
        "  #print(\"search_words\",hashed_url)\n",
        "  for query in query_words:\n",
        "    result = search_engine(url, query)\n",
        "    if result == {} or result is None or result[query] is None:\n",
        "      continue\n",
        "    data_to_upload = {\n",
        "        'count': result[query]\n",
        "    }\n",
        "    #print(hashed_url,\" \",query,\" \",data_to_upload)\n",
        "    result = fbConn.put('/index/' + query,hashed_url,data=data_to_upload)\n",
        "    if result is None:\n",
        "      print(\"search_words - None\")\n",
        "    #print(\"word \",result)"
      ],
      "metadata": {
        "id": "qrxcWy4_u1PV",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Fetch list of words to index\n",
        "def get_words_to_index():\n",
        "    data = fbConn.get('words',None)\n",
        "    terms = [value['term'] for value in data.values()]\n",
        "    return terms\n",
        "\n",
        "print(get_words_to_index())"
      ],
      "metadata": {
        "id": "kYg2xQZFTzOJ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Function to save for the links the urls that navigate to them to be used as an index of relevance\n",
        "\n",
        "def save_links_pointing(url_pointing,links_pointed_at):\n",
        "  try:\n",
        "    for link in links_pointed_at:\n",
        "      hashed_link = hash_url(link)\n",
        "\n",
        "      try:\n",
        "        hashed_pointing= hash_url(url_pointing)\n",
        "        result = fbConn.put(\"pointed/\"+hashed_link+\"/pointed_from\",hash_url(hashed_pointing), data=hashed_pointing)\n",
        "      except:\n",
        "        print(\"Error HABIBI \",url_pointing)\n",
        "  except:\n",
        "    print(\"ERRORRRR\")"
      ],
      "metadata": {
        "id": "4yRUGnWkUhKO",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Scraper Function\n",
        "\n",
        "#this function works as a url BFS search that given a starting node, searchs and build a map (as a graph) of the website it is given\n",
        "# this is also a SINGLE Site Scraper where it ensures to not leave the domain that it startd with\n",
        "def scrape_bfs(start_url):\n",
        "\n",
        "  visited = set()\n",
        "\n",
        "  # Initialize a queue with first url\n",
        "  queue = deque([start_url])\n",
        "\n",
        "  #get the words that we will be searching\n",
        "  queries = get_words_to_index()\n",
        "\n",
        "\n",
        "  # Loop until the queue is empty\n",
        "  while queue:\n",
        "    #get next url to search\n",
        "    url = queue.popleft()\n",
        "    # Skip if the URL has been visited before\n",
        "    if url in visited:\n",
        "        continue\n",
        "\n",
        "    # mark visited to prevent returns and infinite loops in the graph\n",
        "    visited.add(url)\n",
        "\n",
        "    #Get all the links in this page\n",
        "    links = extract_links(url)\n",
        "    print(\"Visiting: \",url,\"\\nLinks Extracted: \",len(links))\n",
        "\n",
        "    #searching words and saving them to firebase\n",
        "    search_words(url,queries)\n",
        "\n",
        "    #save to links found the current url as a url that points to it\n",
        "    save_links_pointing(url,links)\n",
        "\n",
        "    # Process link into the queue of links to visit (if its not already there)\n",
        "    for link in links:\n",
        "      absolute_link = urljoin(url, link)\n",
        "\n",
        "      if is_same_domain(start_url, absolute_link) and absolute_link not in visited:\n",
        "        #add link to queue\n",
        "        queue.append(absolute_link)\n",
        "\n"
      ],
      "metadata": {
        "id": "HUFaVvn3g400",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Start Scrape\n",
        "\n",
        "scrape_bfs('https://www.tencent.com/en-us/')"
      ],
      "metadata": {
        "id": "Aw4gTsFGs2pv",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}